[
{"contents": "Hi there,If you are not signed up already for the Zyte Developers Community newsletter, .In this article, Rinki Nag shows us how to\u00a0\u00a0by scraping Google data, Twitter, etc., and creating word-cloud to perform sentiment analysis.Cliff Chew, a frequent user of Singapore libraries faced a lot of problems navigating their app, finding his favorite book only to realize that it was unavailable at his preferred library. So he decided to write a\u00a0\u00a0and create a dashboard to see which books are available at which libraries.Are you interested in investing in the stock market? You\u2019ll find this article by Code_Jedi useful. He created a web scraping project that\u00a0\u00a0from Yahoo finance using Node.js and puppeteer.Any good ML project needs data to run training and testing. Since most ML libraries can handle pandas data frames that can be edited for your model with minimal changes, Tobi Olabode teaches us how to\u00a0\u00a0you need and convert it into a pandas dataframe.Ever downloaded multiple images from Google? Eshita Goel\u2019s project makes the task easier. She uses\u00a0\u00a0from Google Chrome and saves them to a specific folder."},
{"contents": "Up-to-date, trustworthy data from other websites is the rocket fuel that can power every organisation\u2019s successful growth, including your own. You might want to compare pricing of competitors\u2019 products across popular ecommerce sites. You could be monitoring customer sentiment by trawling for name-checks for your brand \u2013 favourable or otherwise \u2013 in news articles and blogs. Or you might be gleaning information about a particular industry or market sector to guide critical investment decisions.A concrete example of where web data plays an increasingly valuable role in the financial services industry is insurance underwriting and credit scoring. There are billions of \u2018credit invisibles\u2019 around the world, in both developing and mature markets. Although these individuals don\u2019t possess a standard credit history, there\u2019s a huge range of \u2018alternative data\u2019 sources out there, helping lenders assess risk and potentially take these individuals on as clients. These sources range from debit card transactions and utility payments to survey responses, social media posts on a particular topic and product reviews. Read our blog that explains  providers with a precise, insightful alternative dataset.Also in the financial sector, hedge fund managers are turning to alternative data \u2013 beyond the scope of conventional sources like company reports and bulletins \u2013 to help inform their investment decisions. We\u2019ve blogged recently about  in this space, and how Zyte can help deliver standards-compliant custom data feeds that complement traditional research methodologies.Data, in short, is the differentiating factor for companies when it comes to understanding customers, knowing what competitors are up to \u2013 or making just about any kind of commercial decisions based on hard facts rather than intuition.The web holds answers to all these questions, and countless more. Think of it as the world\u2019s biggest and fastest-growing research library. There are billions of web pages out there. Unlike a static library, however, many of those pages present a moving target when details like product pricing can change regularly. Whether you\u2019re a developer or a marketing manager, getting your hands on reliable, timely web data might seem like searching for a needle in a huge, ever-changing digital haystack.So you know your business needs web data. What happens next? There\u2019s nothing to stop you collecting data from any website manually by cutting and pasting the relevant bits you need from other websites. But it\u2019s easy to make errors, and it\u2019s going to be fiddly, repetitive and time consuming for whoever\u2019s been tasked with the job. And by the time you\u2019ve gathered all the data you need, there\u2019s no guarantee that the price or availability of a particular product hasn\u2019t changed.For all but the smallest projects you\u2019ll need to turn to some kind of [automated?] extraction solution. Often referred to as \u2018web scraping\u2019,  is the art and science of grabbing relevant web data \u2013 maybe from a handful of pages, or hundreds of thousands \u2013 and serving it up in a neatly organised structure that your business can make sense of.So how does data extraction work? In a nutshell, it makes use of computers to mimic the actions of a human being when they\u2019re finding specific information on a website, quickly, accurately and at scale. Webpages are designed primarily for the benefit of humans. They tend to present information in ways that we can easily process, understand and interact with. If it\u2019s a product page, for example, the name of a book or a pair of trainers is likely to be shown pretty near the top, with the price nearby and probably with an image of the product too. Along with a host of other clues lurking in the HTML code of that webpage, these visual pointers can help a machine pinpoint the data you\u2019re after with impressive accuracy.There are various practical ways to attack the extraction challenge. The crudest is to make use of the wide range of open source scraping tools that are out there. In essence, these are chunks of ready-written code that scan the HTML content of a webpage, pull out the bits you need and file them into some kind of structured output. Going down the open source route has the obvious appeal of being \u2018free\u2019. But it\u2019s not a task for the faint hearted, and your own developers will spend a fair amount of time writing scripts and tweaking off-the-shelf code to meet the needs of a specific job.OK \u2013 it\u2019s time to put all this web scraping theory into practice. Here\u2019s a worked example that illustrates the three key steps in a real-world extraction project.To keep things simple, we are going to use  and  libraries to create our script.As an example, I will be extracting product data from this website: books.toscrape.comThe extraction script will contain two functions:Making requests is an important part of the script: both for finding the product URLs and fetching the product HTML files. So first, let\u2019s start off by creating a new class and add the base URL of the website:Then, let\u2019s create a simple function that will help us make requests:The function, is fairly simple in itself, but in case you want to scale up your requests with proxies, you will only need to modify this part of your code and not all the places where you invoke .Extract product URLsI will only extract products from one category called Travel to get some sample data. Here, the task is basically to find all product URLs on this category page and return them in some kind of iterable format so we have each URL to make a request to:This is what this function does, line by line:We make a normal request to get to the category page (start_url)Create a BeautifulSoup object which will help us parse the HTML of the category pageWe identify that each product URL on the page is available using the specified selectorIterate over the extracted links - which are at this point are <a> elementsExtract the relative URL from the <a> element, by parsing the href attributeConvert the relative URL to absolute URLReturn a generator with the absolute URLsThe other important part of our script is the product extractor function.As you can see above, for the price field I needed to do some cleaning because it contained currency and other characters as well. Luckily, there\u2019s an open source library which can do the heavy lifting for us to parse the price value, it\u2019s called price_parser (created by Zyte):This function returns the price of the product - extracted from text - as a float value.And finally \u2013 this is the main function where we put together extract_urls() and extract_product().There are plenty of pitfalls to negotiate during the course of any web scraping project. One of the biggest challenges comes when you\u2019re trying to extract data at scale.At Zyte we often talk to clients who\u2019ve successfully extracted data from a hundred webpages a day, or a thousand. Surely, they ask, it must be just as easy getting data from a million pages daily?Many websites use \u2018anti-bot\u2019 technology to discourage automated scraping. There are ways round this, the most effective being the use of smart rotating proxies. This is a technique that effectively lulls a target website into thinking it\u2019s being visited innocuously by a human, rather than an extraction script.Here\u2019s an illustration of how Zyte\u2019s  can be integrated into a data extraction script to boost your chances of getting banned.Remember that we created a make_request() function at the beginning so it handles all the requests in the script? Now if we want to use Smart Proxy Manager, we only need to make a small change in this function. Everything else will work just fine. To integrate Smart Proxy Manager, change this function:to this:In this code, we add the Smart Proxy Manager endpoint as a proxy and authenticate using the Zyte apikey.If you want to learn more about , check out our webinar.At Zyte we\u2019ve spent the best part of a decade focused on extracting the all-important web data that companies need. Our international team of developers and data scientists includes some of the biggest brains in analytics, AI and machine learning. And along the way we\u2019ve developed some powerful tools \u2013 several of them protected by international patents \u2013 to help our customers achieve their data extraction goals quickly, reliably and cost efficiently."},
{"contents": "This month, in Van Buren v. United States, the US Supreme Court ruled that accessing permissible areas of a computer system with prior authorization, even for an improper or prohibited purpose, is not a violation of the CFAA. This is a big win for the web scraping community, as clarity on the breadth of this law has been something many web scrapers have been waiting for. So let\u2019s talk a bit about what exactly this means for web scrapers . . .\u00a0The CFAA, which stands for the Computer Fraud and Abuse Act, is a United States law that many feared could be interpreted as making web scraping a criminal act. This law, which is often referred to as the federal \u201cAnti-Hacking Statute\u201d, was put into effect in the 80s at a time when the landscape of the internet was very different. One can be criminally liable under the CFAA for intentionally accessing a computer without authorization or exceeding authorized access. The scope of the CFAA is very broad, as it encompasses information from every computer connected to the internet, with computer being defined as most electronic processing devices. Violation of the CFAA could result in large fines and/or up-to 10 years of jail time, as well as being included as a private cause of action in a lawsuit. Cybercrime has changed dramatically since 1986 and courts struggled with how to properly apply the CFAA to today\u2019s reality. Until last week, the application of the CFAA varied across jurisdictions of the United States. Given the possible jail time, it\u2019s easy to see why the uncertainty of the scope of the CFAA was a concern for web scrapers.\u00a0 The United States Supreme Court laid the disagreement to rest this month in what can be seen as a victory for web scrapers.\u00a0Nathan Van Buren was a police officer in Georgia. As a part of his job, Van Buren had access to the police database with his own valid credentials. The police department had a policy of prohibiting use of the station database for personal reasons, which they described as an improper purpose. Van Buren was briefed on this policy but still used the police database to search for a license plate outside of the scope of his work in exchange for money. The US Government viewed this violation of the police station\u2019s policy as a breach of the CFAA and charged him with a felony under the clause of the CFAA that makes it a punishable offense to intentionally access a computer without authorization or exceed authorized access to obtain information from a protected computer. Van Buren was found guilty and appealed to the Eleventh Circuit, who shared the Government\u2019s view. Van Buren then appealed to the Supreme Court, arguing that he did not exceed his authorized access as he was entitled to obtain the information, which should not be a violation under the CFAA- a view that many other courts in the US agreed with.\u00a0Prior to the Van Buren ruling, there were two conflicting positions on how to interpret the CFAA.\u00a0 The first, which the Government and Eleventh Circuit took in this case, is that any misuse of access to a computer system for inappropriate reasons is a violation of the CFAA. The second position, which Van Buren argued, is that the CFAA is only violated if information is obtained is information that a user does not have access to. This second position would reduce the scope of the CFAA to hackers and other bad actors. If the first position were applied, violating your employer\u2019s internal policy or a minor breach to a website\u2019s terms of service could subject you to criminal charges and possible jail time. The Supreme Court agreed that this interpretation would \u201ccriminalize everything from embellishing an online-dating profile to using a pseudonym on Facebook\u201d and \u201cattach criminal penalties to a breathtaking amount of commonplace computer activity.\u201d\u00a0In applying this clarified reading of the CFAA, there are two questions to ask here:Van Buren was authorized to access the police database on his patrol-car computer and his credentials gave him access to the information. Even though his use of the police database was improper and outside the scope of his work, the Supreme Court ruled there was no violation of the CFAA as he had authorization to access that information. The circumstances do not matter: you either have access or you don\u2019t. There is still an ongoing case to follow between LinkedIn and HiQ, which the Supreme Court sent back to the Ninth Circuit for a decision in light of the Van Buren ruling. It is important to note that the Supreme Court specifically put off the question of whether access is determined by technological or code-based limitations, or whether access could also be limited by contracts (such as terms of service) or policies. It is unlikely the Supreme Court will come back to this question in the near future, so the LinkedIn case could give us further insight on how this will be interpreted by courts moving forward. Van Buren is a great stride forward for web scraping and we hope to see the Ninth Circuit follow suit and continue on this path toward open access to web data for all.\u00a0Remember, I\u2019m a lawyer but I\u2019m not your lawyer, so it\u2019s always best to get your legal advice from your lawyer. If you are concerned about how the Van Buren case directly affects your practices, check with your lawyer. There are still other considerations regarding the legality of web scraping.\u00a0For more on that, please take a look at our previous blog posts:\u00a0"},
{"contents": "Hi there,If you are not signed up already for the Zyte Developers Community newsletter, .If you\u2019re looking for hotel recommendations, Shivam Naik\u2019s\u00a0\u00a0can help. He uses BeautifulSoup to extract data from TripAdvisor and performs some insightful data analysis using pandas and matplotlib.When Michel Floyd decided to put his house on the market, he turned to data to\u00a0. He wrote a scraper in python to collect data from Zillow and analyzed multiple data fields like: days to sell, price per sq.ft., etc.Cloud services have revolutionized how we\u2019re working with technology. Jos\u00e9 ties this idea into the world of web scraping by creating a\u00a0.Scrapy is on Discord and it\u2019s the hip new place for Scrapy lovers to hang out.\u00a0.Zyte is looking for a Developer Advocate to show fellow developers the magic and wonder of web data extraction tools and platforms. You\u2019ll get to create all types of content for Zyte and build long-lasting relationships with members of the community.\u00a0."},
{"contents": "Dynamic pricing is a great tool for businesses, especially those in the e-commerce field. A lot of major companies already use web extracted pricing data to formulate pricing strategies, adapt to price variations, spot MAP violations & analyze customer opinions. Adding dynamic pricing to that can add an array of benefits like following the competition, adjusting prices instantly, and easily capturing quantitative metrics about your products to boost revenue.Using dynamic pricing makes perfect sense for the bottom line of your business.If you\u2019re looking to learn more about dynamic pricing and how to make the most of it, I am putting together this basic guide about dynamic pricing and how to unleash its maximum potential.First things first...Dynamic pricing is a strategy that involves selling the same product at different prices to different groups of people and/or at different times. It is based on variable prices rather than fixed prices.Dynamic pricing takes competitive intelligence to the next level by combining competitor pricing data with internal data to make automated pricing decisions. This allows companies to be proactive and regularly adjust their pricing in response to real-time demand, supply, and competitor benchmarks.In simple terms, companies adjust their prices multiple times a day based on factors like changing market trends, competitor prices, and demand. This strategy gives companies the dual advantage of increasing sales and optimizing margins.Now that you know what dynamic pricing is, here\u2019s how to build it.To thrive in a fast-paced market, you\u2019ll need to take a data-driven and agile approach to develop your pricing strategies allowing you to react quickly and stay ahead of the game.However, there\u2019s one thing you\u2019ll need if you want to stay ahead of the curve - Data; in real-time, and at scale.In an ever-changing market, you\u2019re not going to manually monitor hundreds of competitors every few minutes. It would be too time-consuming, expensive, and completely unrealistic.\u00a0That way, all you have to do is identify your competition and set up web scrapers that collect pricing data every few minutes so that you can modify your strategy accordingly.If you\u2019re wondering how to build a web scraping project that allows you to extract real-time data from millions of price points regularly, here\u2019s your answer!Fortunately, multiple open-source and commercial tools and libraries are available to help you easily get the web data you need. Here are some of my favorites:If you feel like you need a helping hand in your data extraction project or you\u2019d prefer to leave the data extraction to experts and focus solely on making strategic pricing decisions, .Here at Zyte, we specialize in delivering custom pricing data explicitly designed to make your revenue operations simple and efficient by providing product and pricing datasets from retailer sites and marketplaces of your choice. Quickly and reliably."},
{"contents": "Hi there,If you are not signed up already for the Zyte Developers Community newsletter, .Miguel Magalh\u00e3es was having a hard time finding covid vaccine slots in France. So he built a\u00a0\u00a0that checked the server for the next available slot, until it finds a vacancy.This one is a bit long but interesting read. Brendan Ferris uses\u00a0\u00a0like username, title, content link, etc. from posts on\u00a0old.reddit.com.In this article, Elliot discusses the Wikipedia module in Python which allows you to\u00a0\u00a0in the easiest way possible.Dateparser was developed to make date extraction from HTML pages easier. Initially, used by web scraping developers, it was quickly adopted by the wider community and has been used for multiple applications like command-line tools, chatbots, etc. This blog post covers\u00a0.This article is for those among us who are just starting to get acquainted to web scraping. Runjot Kaur's\u00a0\u00a0is very easy to understand."},
{"contents": "If you are interested in web scraping as a hobby or you might already have a few scripts extracting data but are not familiar with  then this article is meant for you. I\u2019ll go quickly over the fundamentals of Scrapy and why I think it\u2019s the right choice when it comes to scraping at scale. I hope you\u2019ll see the value you can get quickly with this awesome framework and that you\u2019ll be interested in learning more and consider it for your next big project.Scrapy is a web scraping framework written in Python. You can leverage Python\u2019s rich data science ecosystem along with Scrapy, which makes development a lot easier.While the introduction does it justice, this short article aims to show you how much value you can get out of Scrapy and aims to introduce you to a couple of its fundamental concepts. This is not an introduction to web scraping or to Python, so I\u2019ll assume you have basic knowledge of the language and at least an understanding of how HTTP requests work.If you did a Python web scraping tutorial before, chances are you\u2019ve run into the BeautifulSoup and requests libraries. These offer a fast way to extract data from web pages but don\u2019t provide you with the project structure and sane defaults that Scrapy uses for most tasks. You\u2019d have to handle redirects, retries, cookies, and more on your own while Scrapy handles these out of the box.You may think you can get away with a headless browser such as Selenium or Puppeteer, after all, that would be much harder to block. Well, the truth is you could get away with spending a lot fewer resources, which will take a toll if you have hundreds or thousands of scrapers.Scrapy is a Python package like any other. You can install with pip in your virtualenv like so:The two concepts you need to understand are the Scrapy project and the spider. A project wraps multiple spiders and you can think of a spider as a scraping configuration for a particular website. After installing, you can start a project like so:A project will encapsulate all your spiders, utilities, and even the deployment configs.A spider handles everything needed for a particular website. It will yield requests to web pages and receive back responses. Its duty is to then process these responses and yield either more requests or data.In actual Python code, a spider is no more than a Python class that inherits from . Here\u2019s a basic example:The  is a list of URLs to start scraping from. Each will yield a request whose response will be received in a callback. The default callback is . As you can see, callbacks are just class methods that process responses and yield more requests or data points.You can use Scrapy's selectors! There are CSS selectors available directly on the  object for this:There are also XPath selectors, which offer more powerful options that you\u2019ll most likely need. Here are the same selectors using XPath:Next, you\u2019ll need a way to return your data into a parsable format. There are powerful utilities, such as items and item loaders, but in its simplest form, you can store your data into Python dictionaries:In your project directory, using the above example project, you can run:This will display the scraped data to the standard output along with a lot of logging but you can easily redirect only the actual data to CSV or to JSON format by adding a couple more options:Contents of CSV file:Scrapy makes it easy to manage complex session logic. As you add more spiders and your project gets more complex, Scrapy allows you to prevent bans in various ways.The most basic way to tweak your requests is to set headers. For example, you can add an  header like so:You may think already that there must be a better way of setting this than doing it for each individual request, and you\u2019re right! Scrapy lets you set default headers and options for each spider like this:This can either be set on individual spiders or in your  file which Scrapy defines for you.You can also use  to do this. These can be used across spiders to add headers and more.Middlewares are another powerful feature of Scrapy because they allow you to do things like handling redirects, retries, cookies, and more. And that\u2019s just what Scrapy has out of the box!\u00a0Using middlewares you can  for particular websites to ensure that you don\u2019t crawl something you shouldn\u2019t.\u00a0Web scraping can take a toll on the website which is not what you intended. To , you\u2019ll need to add sane delays between your requests. You can easily do that using the existing .\u00a0You can also set an interval so you don\u2019t look like a bot by requesting precisely every 2 seconds but yielding a request anywhere from 1 to 5 seconds!There are many ways to work with proxies in Scrapy. You can set them for individual requests like so:Or using the existing , to set it for each individual request.If you\u2019re using Smart Proxy Manager () you can use the  to set it up.Scrapy also offers you items to help define a structure for your data. Here\u2019s how a simple definition looks:You can use data classes as well! are the next step for data formatting. To understand where they become useful, you can think of multiple spiders using the same item and requiring the same formatting. For example, stripping spaces of the \u2018description\u2019 field and merging the list of strings. They can do some pretty complex stuff!Pipelines for processing items are also an option. They can be used for filtering duplicate items based on certain fields or add/validate computed values (such dropping items based on timestamp).This was just an overview, there are multiple other features included directly in Scrapy as well as many extensions, middlewares, and pipelines created by the community. Here\u2019s a shortlist of resources you may be interested in:Scrapy is a mature open source project with many active contributors and has been around for years. It\u2019s well supported so you\u2019ll find  and  for almost everything you can think of and there are lots of plugins developed by the community."},
{"contents": "Hi there,If you are not signed up already for the Zyte Developers Community newsletter, .Roy Healy introduces two packages - chompjs and JMESPath that you can use to easily extract web data from dynamic pages. Read his\u00a0. If you prefer watching a\u00a0, you can do that too.Jose writes this article to build and present one viable implementation of a\u00a0\u00a0using\u00a0\u00a0for use in scrapy pipelines.John Watson Rooney created this interesting product scraping video to build an advanced Shopify scraper that extracts the products and saves them onto an SQL database.\u00a0.When web-scraping with python, selenium has proved to be very efficient. It is really useful when web pages load content only upon scrolling but this also brings its fair share of trouble with web scrapers. Zihan has a\u00a0\u00a0for it!"},
{"contents": "Hi there,If you are not signed up already for the Zyte Developers Community newsletter, .In this tutorial, Bilge Demirkaya  on Decathalon.com using the Scrapy-Splash plugin.Have you been waiting for a price drop on an item in your wishlist and want to be the first person to be notified of its price drop? Yashashree Suresh explains how to  using a custom clock process on Heroku for free.In this blog, Raghavendra Basvan shows us how to  by applying various methods from pandas and how to analyze the data using matplotlib and seaborn modules.If you\u2019re a book lover, you'll find this fun project very interesting. Alexandre wanted to , so he decided to undertake a web scraping project to keep a tab on his purchases, the book categories, and pages he\u2019s read using Python.Konstantin Lopukhin, an expert in improving the data quality of automatic data extraction, takes you on his journey of measuring Zyte\u2019s product data extraction quality and shares the results from the competitive comparison. Packed with insights, this live webinar will tell you more about the learning and methodology he followed. ."},
{"contents": "In this tutorial, you will learn how to scale up your already existing Scrapy project in order to make more requests and extract more web data. is a very popular web crawling framework and can make your life so much easier if you\u2019re a web data extraction developer. Scrapy can handle many web scraping jobs including URL discovery, parsing, data cleaning, custom data pipelines, etc\u2026 But there\u2019s one thing that Scrapy cannot do out of the box and it has become a must if you want to extract large amounts of data reliably: proxy management.In order to scale up your Scrapy project, you need a proxy solution.I will show you how to turn your already existing Scrapy spider and boost it with proxies!For this example, I\u2019m going to use the \u201cScrapy version\u201d of the product extractor spider that contains two functions:Here\u2019s the Scrapy spider code:So let\u2019s assume this is the spider you currently have and it was working fine\u2026 delivering you precious data\u2026 for a while. Then you wanted to scale up and make more requests which ultimately led to blocks, low success rate, bad data quality, etc\u2026\u00a0The solution to overcome blocks and receive high-quality web data is proxies and how you use those proxies. Let\u2019s see how you can integrate Smart Proxy Manager in this spider!The recommended way for integration is the . This is how you can install it:Then, add these settings in your Scrapy project file:Notice that in order to use Smart Proxy Manager, you need an API key. But don\u2019t worry, we offer a  (max 10K requests during the trial) so you can get your API key fast and try it out to make sure it works for you.Optionally, you can also set the proxy URL if you requested a custom instance of Smart Proxy Manager:To achieve higher crawl rates when using Smart Proxy Manager, we recommend disabling the Auto Throttle addon and increasing the maximum number of concurrent requests. You may also want to increase the download timeout. Here is a list of settings that achieve that purpose:This simple integration takes care of everything you need to scale up your Scrapy project with proxies.If you are tired of blocks or managing different proxy providers, "}
]